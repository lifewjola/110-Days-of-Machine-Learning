{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CLASSIFICATION MODELS CHEAT SHEET\n",
        "This notebook contains:\n",
        "- 7 most common classification models\n",
        "- How to know when they're ideal for a problem\n",
        "- The ind of data to use them with\n",
        "- Use cases/ scenario where they're most suitable\n",
        "- Their syntaxes (for importing, fitting, testing, evaluating)"
      ],
      "metadata": {
        "id": "MGlHfx2eUH8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **Logistic Regression**:\n",
        "   - How to know it's ideal: Suitable for binary or multi-class classification problems with linear decision boundaries.\n",
        "   - Data Type: Numerical and categorical data (after encoding).\n",
        "   - Syntax:\n",
        "   ```python\n",
        "   from sklearn.linear_model import LogisticRegression\n",
        "   model = LogisticRegression()\n",
        "   model.fit(X_train, y_train)\n",
        "   y_pred = model.predict(X_test)\n",
        "   ```\n",
        "   - Use Cases/Scenarios: Customer churn prediction, email spam detection, medical diagnosis.\n",
        "   - Best Evaluation Metrics: Accuracy, Precision, Recall, F1-score, Area Under the ROC Curve (AUC-ROC).\n",
        "   - Metric Syntax and Graphs:\n",
        "   ```python\n",
        "   from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
        "   accuracy = accuracy_score(y_test, y_pred)\n",
        "   precision = precision_score(y_test, y_pred)\n",
        "   recall = recall_score(y_test, y_pred)\n",
        "   f1 = f1_score(y_test, y_pred)\n",
        "   auc_roc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "   fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
        "   cm = confusion_matrix(y_test, y_pred)\n",
        "   # Plot ROC Curve\n",
        "   import matplotlib.pyplot as plt\n",
        "   plt.plot(fpr, tpr)\n",
        "   plt.xlabel('False Positive Rate')\n",
        "   plt.ylabel('True Positive Rate')\n",
        "   plt.title('ROC Curve')\n",
        "   plt.show()\n",
        "   ```\n",
        "\n",
        "2. **K-Nearest Neighbors (KNN)**:\n",
        "   - How to know it's ideal: Suitable for problems with a well-defined distance metric and non-linear decision boundaries.\n",
        "   - Data Type: Numerical data.\n",
        "   - Syntax:\n",
        "   ```python\n",
        "   from sklearn.neighbors import KNeighborsClassifier\n",
        "   model = KNeighborsClassifier(n_neighbors=k)\n",
        "   model.fit(X_train, y_train)\n",
        "   y_pred = model.predict(X_test)\n",
        "   ```\n",
        "   - Use Cases/Scenarios: Image classification, text classification, recommendation systems.\n",
        "   - Best Evaluation Metrics: Accuracy, Precision, Recall, F1-score, AUC-ROC.\n",
        "   - Metric Syntax and Graphs: Same as Logistic Regression.\n",
        "\n",
        "3. **Decision Trees**:\n",
        "   - How to know it's ideal: Suitable for problems with complex decision boundaries and interpretable results.\n",
        "   - Data Type: Numerical and categorical data (after encoding).\n",
        "   - Syntax:\n",
        "   ```python\n",
        "   from sklearn.tree import DecisionTreeClassifier\n",
        "   model = DecisionTreeClassifier()\n",
        "   model.fit(X_train, y_train)\n",
        "   y_pred = model.predict(X_test)\n",
        "   ```\n",
        "   - Use Cases/Scenarios: Medical diagnosis, credit risk analysis, customer segmentation.\n",
        "   - Best Evaluation Metrics: Accuracy, Precision, Recall, F1-score, AUC-ROC.\n",
        "   - Metric Syntax and Graphs: Same as Logistic Regression.\n",
        "\n",
        "4. **Random Forest**:\n",
        "   - How to know it's ideal: Suitable for high-dimensional data and problems with complex decision boundaries. Provides better generalization and reduces overfitting compared to decision trees.\n",
        "   - Data Type: Numerical and categorical data (after encoding).\n",
        "   - Syntax:\n",
        "   ```python\n",
        "   from sklearn.ensemble import RandomForestClassifier\n",
        "   model = RandomForestClassifier(n_estimators=num_estimators)\n",
        "   model.fit(X_train, y_train)\n",
        "   y_pred = model.predict(X_test)\n",
        "   ```\n",
        "   - Use Cases/Scenarios: Predicting customer churn, image recognition, anomaly detection.\n",
        "   - Best Evaluation Metrics: Accuracy, Precision, Recall, F1-score, AUC-ROC.\n",
        "   - Metric Syntax and Graphs: Same as Logistic Regression.\n",
        "\n",
        "5. **Support Vector Machines (SVM)**:\n",
        "   - How to know it's ideal: Suitable for binary and multi-class classification problems with a clear margin of separation between classes.\n",
        "   - Data Type: Numerical data.\n",
        "   - Syntax:\n",
        "   ```python\n",
        "   from sklearn.svm import SVC\n",
        "   model = SVC(kernel='linear')\n",
        "   model.fit(X_train, y_train)\n",
        "   y_pred = model.predict(X_test)\n",
        "   ```\n",
        "   - Use Cases/Scenarios: Text categorization, hand-written digit recognition.\n",
        "   - Best Evaluation Metrics: Accuracy, Precision, Recall, F1-score, AUC-ROC.\n",
        "   - Metric Syntax and Graphs: Same as Logistic Regression.\n",
        "\n",
        "6. **Naive Bayes**:\n",
        "   - How to know it's ideal: Suitable for text classification tasks and when the \"naive\" assumption of feature independence holds.\n",
        "   - Data Type: Textual or categorical data (after converting to numerical representations such as BoW, TF-IDF, or word embeddings).\n",
        "   - Syntax:\n",
        "   ```python\n",
        "   from sklearn.naive_bayes import MultinomialNB\n",
        "   model = MultinomialNB()\n",
        "   model.fit(X_train, y_train)\n",
        "   y_pred = model.predict(X_test)\n",
        "   ```\n",
        "   - Use Cases/Scenarios: Email spam detection, sentiment analysis, text classification.\n",
        "   - Best Evaluation Metrics: Accuracy, Precision, Recall, F1-score, AUC-ROC.\n",
        "   - Metric Syntax and Graphs: Same as Logistic Regression.\n",
        "\n",
        "7. **Gradient Boosting Machines (GBM)**:\n",
        "   - How to know it's ideal: Suitable for binary and multi-class classification problems, particularly when high accuracy is desired.\n",
        "   - Data Type: Numerical and categorical data (after encoding).\n",
        "   - Syntax:\n",
        "   ```python\n",
        "   from sklearn.ensemble import GradientBoostingClassifier\n",
        "   model = GradientBoostingClassifier(n_estimators=num_estimators)\n",
        "   model.fit(X_train, y_train)\n",
        "   y_pred = model.predict(X_test)\n",
        "   ```\n",
        "   - Use Cases/Scenarios: Click-through rate prediction, customer churn prediction.\n",
        "   - Best Evaluation Metrics: Accuracy, Precision, Recall, F1-score, AUC-ROC.\n",
        "   - Metric Syntax and Graphs: Same as Logistic Regression.\n"
      ],
      "metadata": {
        "id": "-H9pkBhUoRnQ"
      }
    }
  ]
}